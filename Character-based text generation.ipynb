{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from keras.callbacks import LambdaCallback, ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import preprocessing\n",
    "from keras import backend as K\n",
    "from keras.utils import Sequence\n",
    "from keras.models import load_model\n",
    "from keras.metrics import categorical_accuracy\n",
    "#from sklearn import model_selection\n",
    "import numpy as np\n",
    "import random\n",
    "import sys\n",
    "import io\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_path = \"text/your_text.txt\"\n",
    "out_path = \"text/your_text_clean.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanitize clean txt\n",
    "charset = ['\\n', ' ', '!', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
    "           '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', \n",
    "           'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "text_cleaner(in_path, charset, out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load clean text from file\n",
    "with io.open(out_path, encoding='utf-8') as f:\n",
    "    textclean = f.read().lower()\n",
    "print('corpus length:', len(textclean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise the corpus and create a dictionary\n",
    "# Character-based: each independent character constitutes a \"word\"\n",
    "text2words = [word for word in textclean]\n",
    "wordlist = sorted(list(set(text2words)))\n",
    "print(\"Total words in corpus: \", len(text2words))\n",
    "print(\"Dictionary length: \", len(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps words <-> indices\n",
    "word_indices = dict((w, i) for i, w in enumerate(wordlist))\n",
    "indices_word = dict((i, w) for i, w in enumerate(wordlist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the sentences of length maxlen that will make up our dataset\n",
    "# The \"y\" for each training sentence is the word that immediately follows\n",
    "maxlen = 40\n",
    "step = 1\n",
    "sentences = []\n",
    "next_words = []\n",
    "for i in range(0, len(text2words) - maxlen, step):\n",
    "    sentences.append(text2words[i: i + maxlen])\n",
    "    next_words.append(text2words[i + maxlen])\n",
    "print('number of sentences:', len(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_splitter(X, Y, percentage):\n",
    "    \n",
    "    indices = np.arange(len(X))\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    test_size = int(percentage * len(X))\n",
    "    test_indices = indices[0:test_size]\n",
    "    train_indices = indices[test_size:]\n",
    "    \n",
    "    X_train = [X[i] for i in train_indices]\n",
    "    X_test = [X[i] for i in test_indices]\n",
    "    \n",
    "    Y_train = [Y[i] for i in train_indices]\n",
    "    Y_test = [Y[i] for i in test_indices]\n",
    "    \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_train, next_words_train, sentences_test, next_words_test = train_test_splitter(sentences, next_words, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for the generator\n",
    "params = {'dim': (maxlen, len(wordlist)),\n",
    "          'batch_size': 128,\n",
    "          'n_classes': 6, # not used here\n",
    "          'n_channels': 1, # not used here\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, sentences, next_words, batch_size=64, dim=(maxlen, len(wordlist)), n_channels=1,\n",
    "                 n_classes=10, shuffle=True):\n",
    "        'Initialization'\n",
    "        self.dim = dim\n",
    "        self.batch_size = batch_size\n",
    "        self.next_words = next_words\n",
    "        self.sentences = sentences\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return int(np.floor(len(self.sentences) / self.batch_size))\n",
    "        \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Find list of IDs\n",
    "        sentences_temp = [self.sentences[k] for k in indexes]\n",
    "        next_words_temp = [self.next_words[k] for k in indexes]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(sentences_temp, next_words_temp)\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(len(self.sentences))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def __data_generation(self, sentences_temp, next_words_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        X = np.zeros((self.batch_size, *self.dim), dtype=np.bool)\n",
    "        y = np.zeros((self.batch_size, len(wordlist)), dtype=np.bool)\n",
    "        \n",
    "        wordindex = 0\n",
    "                 \n",
    "        for i in range(self.batch_size):\n",
    "            for t, w in enumerate(sentences_temp[i]):\n",
    "                X[i, t, word_indices[w]] = 1\n",
    "            y[i, word_indices[next_words_temp[wordindex]]] = 1\n",
    "\n",
    "            wordindex += 1\n",
    "            if wordindex == len(sentences):\n",
    "                wordindex = 0\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoder\n",
    "\n",
    "def encoder(X, Y, word_indices):\n",
    "    X_enc = np.zeros((len(X), maxlen, len(wordlist)), dtype = np.bool)\n",
    "    Y_enc = np.zeros((len(X), len(wordlist)))\n",
    "    \n",
    "    wordindex = 0\n",
    "    \n",
    "    for i in range(len(X)):\n",
    "            for t, w in enumerate(X[i]):\n",
    "                X_enc[i, t, word_indices[w]] = 1\n",
    "            Y_enc[i, word_indices[Y[wordindex]]] = 1\n",
    "\n",
    "            wordindex += 1\n",
    "            #if wordindex == len(sentences):\n",
    "                #wordindex = 0\n",
    "\n",
    "    return X_enc, Y_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_test_oh, next_words_test_oh = encoder(sentences_test, next_words_test, word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model: 2 LSTMs with dropout, Adam optimization\n",
    "print(\"Building the model\")\n",
    "model = Sequential()\n",
    "model.add(LSTM(128, return_sequences=True, input_shape=(maxlen, len(wordlist))))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(wordlist)))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = Adam(lr=0.001)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics = [categorical_accuracy])\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to sample an index from the probability output\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prints a test run after each epoch\n",
    "# \"Diversity\" is the temperature, or how far the sampling can stray from the most likely choice\n",
    "def on_epoch_end(epoch, _):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = random.randint(0, len(text2words) - maxlen - 1)\n",
    "    for diversity in [0.2, 0.3, 0.5, 1]:\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = text2words[start_index: start_index + maxlen]\n",
    "        \n",
    "        generated = ''.join(sentence)\n",
    "\n",
    "        print('----- Generating with seed: \"' + ''.join(sentence) + '\"')\n",
    "        sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(40):\n",
    "            x_pred = np.zeros((1, maxlen, len(wordlist)))\n",
    "            for t, word in enumerate(sentence):\n",
    "                x_pred[0, t, word_indices[word]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "\n",
    "            #generated += ' '\n",
    "            generated += str(next_word)\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "\n",
    "            sys.stdout.write(next_word)\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n",
    "checkpoint_callback = ModelCheckpoint(\"your_model_name.h5\", monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "training_generator = DataGenerator(sentences_train, next_words_train, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = load_model(your_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(generator=training_generator, epochs=10,\n",
    "                    use_multiprocessing=False,\n",
    "                    workers=10,\n",
    "                   callbacks=[print_callback, checkpoint_callback],\n",
    "                   validation_data=(sentences_test_oh, next_words_test_oh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.save(your_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feeds some input text to the trained model, and outputs its completion\n",
    "\n",
    "def inputsample(input_text, div = [0.3], num=140):\n",
    "    \n",
    "    sentence = [letter for letter in input_text]\n",
    "    \n",
    "    for diversity in div:\n",
    "        sentence = [letter for letter in input_text]\n",
    "        \n",
    "        print(\"Diversity\", diversity, \"\\n\")\n",
    "        \n",
    "        generated = ''.join(sentence)\n",
    "        # zero pad the sentence to maxlen characters.\n",
    "        while len(sentence) < maxlen:\n",
    "            sentence.append('éàé')\n",
    "\n",
    "        #sys.stdout.write(input_text)\n",
    "        for i in range(num):\n",
    "\n",
    "            x_pred = np.zeros((1, maxlen, len(wordlist)))\n",
    "\n",
    "            for t, word in enumerate(sentence):\n",
    "                if word != 'éàé':\n",
    "                    x_pred[0, t, word_indices[word]] = 1.\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)[0]\n",
    "            next_index = sample(preds, diversity)\n",
    "            next_word = indices_word[next_index]\n",
    "\n",
    "            \n",
    "            generated += str(next_word)\n",
    "            sentence = sentence[1:]\n",
    "            sentence.append(next_word)\n",
    "\n",
    "            #sys.stdout.write(next_word)\n",
    "            #sys.stdout.flush()\n",
    "        print(generated)\n",
    "        print(\"\\n -------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Enter some input text (up to 40 characters including spaces)\n",
    "input_text = \"i believe that\"\n",
    "# Choose an array of diversities to try out\n",
    "div = [0.3]\n",
    "# Choose length of the output text\n",
    "length = 140\n",
    "inputsample(input_text, div, length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf_gpu]",
   "language": "python",
   "name": "conda-env-tf_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
